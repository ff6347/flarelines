# Journal 2026-01-02

## iOS App for On-Device Model Testing

### What We Did

Built a minimal iOS app to test the fine-tuned Qwen2.5-3B lupus diary scoring model on-device using llama.cpp.

### Steps Completed

1. **Downloaded llama.cpp XCFramework (b7611)**
   - Latest release from `https://github.com/ggml-org/llama.cpp/releases`
   - Includes iOS arm64, simulator, macOS, tvOS, visionOS targets

2. **Created Swift files for iOS app**
   - `LlamaContext.swift` - Actor-based wrapper around llama.cpp C API
   - `ContentView.swift` - SwiftUI interface for diary entry and scoring
   - `LupusDiaryApp.swift` - App entry point

3. **Set up Xcode project (WolfsbitGGUFTest)**
   - Added llama.xcframework with Embed & Sign
   - Added Metal.framework and Accelerate.framework
   - Declined bridging header (not needed - framework has modulemap)

4. **Fixed Swift/C interop issues**
   - Changed `sampler` type from `OpaquePointer?` to `UnsafeMutablePointer<llama_sampler>?`
   - Replaced `llama_batch_clear` and `llama_batch_add` C macros with Swift implementations
   - C macros don't bridge to Swift - they're preprocessor directives, not functions

### Technical Insights

**Why C macros don't work in Swift:**
- `llama_batch_add` and `llama_batch_clear` are `#define` macros in llama.h
- Swift can only import actual C functions, not preprocessor macros
- Solution: Manually manipulate the `llama_batch` struct fields in Swift

**Batch manipulation replacement:**
```swift
// Instead of: llama_batch_clear(&batch)
batch.n_tokens = 0

// Instead of: llama_batch_add(&batch, token, pos, seq_ids, logits)
batch.token[i] = token
batch.pos[i] = pos
batch.n_seq_id[i] = Int32(seqIds.count)
batch.seq_id[i]![j] = seqId
batch.logits[i] = logits ? 1 : 0
batch.n_tokens += 1
```

### Files Created/Modified

**New in wolfsbit-01/ios-app/ (templates):**
- `llama.xcframework/` - llama.cpp b7611
- `LlamaContext.swift`
- `ContentView.swift`
- `LupusDiaryApp.swift`
- `README.md`

**Actual Xcode project:**
- `/Users/tomato/Documents/apps/WolfsbitGGUFTest/WolfsbitGGUFTest/`

### Project Status

| Component | Status |
|-----------|--------|
| Dataset (33 entries, imbalanced) | Needs synthetic data |
| Training pipeline | Complete |
| GGUF export (q4_k_m, 1.8GB) | Complete |
| iOS app scaffold | Complete, builds successfully |

### On-Device Testing - Success

Added GGUF model to Xcode project and ran on device. Model loads and generates scores.

**Issue encountered:** Filename had " 2" suffix from macOS copy (`qwen2.5-3b-diary-q4_k_m 2.gguf`). Renamed to match code expectation.

### Discovery: Synthetic Data Not Used in Training

Investigated why model might be biased. Found:

**Synthetic data exists and is balanced:**
```
data/processed/synthetic-20251208-095929-n100-d25-25-25-25.jsonl
  Score 0: 25
  Score 1: 25
  Score 2: 25
  Score 3: 26
```

**But training only used original data:**
- Trainer state shows 48 steps, 3 epochs, batch size 2
- 48 / 3 / 2 = ~32 samples = original `dataset.jsonl` (33 entries)
- Synthetic 100 entries were NOT included

**Current model is trained on imbalanced data (72% score 1, 0% score 2).**

### Retrained with Balanced Data

Retrained model including synthetic data (~133 samples total, balanced across scores 0-3).

### GGUF Conversion Steps

Documented the full pipeline for future reference:

```bash
# 1. Merge adapter (from ml-training)
cd /Users/tomato/Documents/fine-tuning/wolfsbit-01/ml-training
uv run python src/merge_adapter.py \
    --adapter outputs/lora-adapter \
    --output outputs/merged-model

# 2. Convert to GGUF (from llama.cpp)
cd /Users/tomato/Documents/fine-tuning/llama.cpp
.venv/bin/python convert_hf_to_gguf.py \
    ../wolfsbit-01/ml-training/outputs/merged-model \
    --outfile ../wolfsbit-01/ml-training/outputs/qwen2.5-3b-diary-f16.gguf \
    --outtype f16

# 3. Quantize to Q4_K_M (1.8GB, good for mobile)
./build/bin/llama-quantize \
    ../wolfsbit-01/ml-training/outputs/qwen2.5-3b-diary-f16.gguf \
    ../wolfsbit-01/ml-training/outputs/qwen2.5-3b-diary-q4_k_m.gguf \
    Q4_K_M
```

### Retrained with Fixed Format

Retrained after fixing synthetic data prompt format. Model now uses consistent German format.

### Testing Results

Model still struggles with score 1 vs 2 boundary:
- "starke Gelenkschmerzen musste Ibuprofen nehmen" → returns 1 (should be 2)
- "Sehr starke Gelenkschmerzen..." → returns 2 (correct)

**Root cause:** Original dataset has ZERO score 2 examples. All score 2 training comes from synthetic data (25 entries). The model hasn't learned real-world score 2 patterns.

### Added mise tasks for export pipeline

```bash
mise run merge-adapter   # Merge LoRA → base
mise run convert-gguf    # → GGUF f16
mise run quantize-gguf   # → Q4_K_M
mise run export-model    # All three chained
```

### Future Work

- Improve score 1 vs 2 discrimination:
  - Option A: Add scoring criteria to app system prompt
  - Option B: Generate more diverse score 2 synthetic data
  - Option C: More training epochs
  - Option D: Manually curate real score 2 examples

---

## Handoff

### Current State

Full pipeline working: training → GGUF export → iOS app. Model runs on device. Score 0, 1, 3 work reasonably well. Score 2 discrimination needs improvement.

### Key Paths

| What | Path |
|------|------|
| GGUF model | `ml-training/outputs/qwen2.5-3b-diary-q4_k_m.gguf` (1.8GB) |
| Xcode project | `/Users/tomato/Documents/apps/WolfsbitGGUFTest/` |
| Original data | `data/processed/dataset.jsonl` (33 entries, 0 score 2) |
| Synthetic data | `data/processed/synthetic-*-n100-*.jsonl` (100 entries, balanced) |
| llama.cpp | `/Users/tomato/Documents/fine-tuning/llama.cpp/` |

### Quick Commands

```bash
mise run train -- --dataset ../data/processed/  # Train
mise run export-model                            # Merge + GGUF + quantize
```

### Known Issue

Score 1 vs 2 boundary is weak because original data has zero score 2 examples. See "Future Work" above for options.
